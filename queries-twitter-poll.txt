weird algorithms
================

handwriting recognition:
   I‚Äòve implemented an ancient handwriting recognition algorithm in
   (recursive) SQL for a seminar at university.‚Ä¶ https://t.co/pAENb5XdiN
leap year:       I once wrote a SQL function to determine if a year is a leap year.
???:             Compute relatedness in a pedigree data (fun) NHS number validator (fun) Merging algorithm for NHS A&amp;E and N‚Ä¶ https://t.co/2HclOt8gNm
pagerank:         I did PageRank for a class once. Astonishingly takes only 30 ish lines
css font maching : I implemented the standard css font matching algorithm in sql, including the part where font weights have to‚Ä¶ https://t.co/XJUVIqjkCk
turing machine   : I once implemented a Turing machine in Oracle SQL
   https://t.co/1Jgzv4QUrc. I also once implemented the supplies replenishment
   logic for a car factory in one large SQL statement (about 20 pages of code).
   It was magnitudes faster and had way less bugs than the iterative version.
   But it was just as impossible to understand :-/
fft:              One of my favorites, but the work of @alberto_dellera, was his Fast Fourier Transform using the MODEL clause - https://t.co/QFgfRC3uFp
crc32:            My teammate wrote a SQL UDF this week to compute a CRC32 that left me in utter awe: https://t.co/7n3JCEUEW0
tf-idf:
    * Fun: calculating interestingness measure like tf-idf and (weighted) log odds in different subgroups
    * Wrote a search engine UDF from scratch in PL/pgSQL. It implemented the TF-IDF and Cosine Similarity formulas‚Ä¶ https://t.co/D8J14c54AV
stats stuff:
    * More recently was implementing Spearman's Ranked Correlation in SQL.
    * Apache #MADlib began as writing Naive Bayes in SQL. Thats fun. Then linear regression. More fun. Then what the heck all of machine learning.
k means:
    *  Recursive K means clustering... it broke production (query on dedicated analytics server blocked writes from‚Ä¶ https://t.co/bqjnxJr8q7
clock(?):
   * The #PostGIS clock :-) https://t.co/lMZkvmyjgc

cool sql tricks
===============

*  My brilliant coworker figured out that she could simulate throwing an exception in a series of SQL queries by‚Ä¶ https://t.co/NUQRxyL8ug

CTEs
====

*  I‚Äôve loved using CTEs! Also, I‚Äôve written queries using MERGE and the APPLY operators using TSQL
*  Definitely a recursive common table expression. Essentially just cribbed it from the SQLite docs and tweaked‚Ä¶ https://t.co/WRKQ83MuKU
*  A recursive drinking cte that fit in the old 140 character tweets. https://t.co/d04U5y0D02
* g @b0rk or use a CTE which is essentially a view defined inside a select query
*  validation of a three-tier deep hierarchical data model, to include in a listing query, many CTEs were written that day
*  An CTE which went to the last root, till only an employee was found.
* nw @b0rk I really like this one, used to audit a dataset. It uses CTEs to run various test queries and hstore‚Ä¶ https://t.co/f0IEeKKsIu
*  And the SQL version is too old for CTEs üò≠üò≠üò≠üò≠üò≠üò≠
*  Almost every join over more than two tables or sub-query was hard before I learned about CTEs/with-clauses!
*  LEAD and LAG in sql server are awesome functions. Also windows functions like row_number come in handy. Plus CTEs are awesome.

graph closure:
    *  Transitive graph closure implementation before I knew it was called like that...
    *  A recursive CTE to pull all the objects from a graph (the graph is basically a ton of tiny trees of 1-5 nodes)
    *  Using a recursive CTE to traverse a directed graph in one query is fun!  I can't find the thing itself, but... ::loading spinner::
    *  implementing a graph traversal to see which department at a company has the largest head-count
    *  Also I guess technically this is traversing multiple directed graphs, bc disjoint roots, I dunno, my graph theory is super notional.

performance
============

*  For a while I was writing analytic models using Hive only . Which is an SQL dialect for a big data computatio‚Ä¶ https://t.co/rGWG1vIyXi
* Just loading data from a few tables with some joins.... but this was on a very write heavy system so we had to put some extremely specific lock hints to do technically unsafe things in, to avoid deadlocking the database
* Using explain to spot a problem and by adding an index or a hint (in oracle) reduces a query from 5 minutes to 10 seconds is really rewarding.

not good at sql
===============
* I'll be honest, SQL is not my strength.  I have to relearn basic upserts every time.
* While everybody here is talkong about huge programs and complec stuff that i am yet to achieve, as a beginner, i struggled with GROUP BY because it took me a week to realise we have to use aggregate functions along with it. I hope one day i can do amazing SQL work like y'all.  ‚ù§
*  Honestly, I still die when I have to write a having query with a condition for a precise number of joined rows.

window functions
================
*  oh lordy -- anything with window functions generally breaks my brain.
*  Used the row_id() window function to generate unique IDs to stand in for IPs when exporting data for a customer.
*  Window functions always blow my mind. Stuff like RANK() OVER is so useful.
*  Finding gaps in time stamped records. R0: S0-E0, R1: S1-E1, where S1 &gt; E0.
*  I'm having fun with the OVER clause in SQL Server quite useful when you have to do some tricky updates https://t.co/R9f4MqLCVM

session ids
===========
*  Adding session ids to events! It required some window functions and cases for "if the previous set of events‚Ä¶ https://t.co/SLwhPh3S5D
* Identify all rows associated with a single session, get starting and ending times, use those to establish max concurrency of sessions in a given variable date/time range
*  We had list of entries that was patient‚Äôs records (immunisations,
   prescriptions, etc.) that were ordered by time. What we needed to do is to
   group them into sets that: - starts with the first entry the day - ends with
   the end of the day Simple, but with a catch. One of the possible entries was
   ‚Äúadmission‚Äù which has then changed to ‚Äúdischarge‚Äù, and these entries, and
   all entries between them, needed to be grouped together (aka not end at the
   end of the day). That was meant to be one-time query, but it still was fun
   to write. It ended as ~150 lines of CTE SQL (it also gathered other
   informations like who created the group and so on). Unfortunately I wasn‚Äôt
   the one that ran that as I changed the workplace and passed it down to the
   old colleague of mine. I hope that it worked without problems.
   https://twitter.com/i/web/status/1175317813004779520

pagination
=========

*  This is a great SQL problem. It seems simple, but is in fact very complicated and deep. (pagination) https://t.co/gohqwTr1k3

select except
=============
* This is really basic but whenever I use SELECT... EXCEPT I always feel a fresh wave of amazement at how great it is that I don‚Äôt need to analyse the results myself. Especially on redshift, where it‚Äôs blisteringly fast. Also I recently ran an insert query that added ONE BILLION records to a table. I was expecting it to take ages but redshift didn‚Äôt even break a sweat, it was done in 13 minutes.

join things
===========

*  Beginners seem to have a lot of trouble understanding group by and left join, and their combination. Once you‚Ä¶ https://t.co/riwPgk788h
*  That *is* a join. A semi join https://t.co/F7BweQgybl
*  Sorting by data not in the DB. Basically JOIN against a table literal.
*  Just loading data from a few tables with some joins.... but this was on a very write heavy system so we had t‚Ä¶ https://t.co/tZO2GLO675
*  Inner join on nested subqueries to optimize a slowass query.
*  I had a really badly performing application (using linq to access sql data and joining lots of datasets toget‚Ä¶ https://t.co/8J6DoQcseg
*  I wrote a book recommendation algorithm primarily in SQL: Lots of joining to find "similar" users (people who‚Ä¶ https://t.co/WXIjzWQNJp
*  I love those! So much fun building up a bunch of useful derived tables and then joining them all toge‚Ä¶ https://t.co/DQTaiwm3sR
*  Identifying non-disjoint causal factors in failure over multiple task execution attempts for the same entity,‚Ä¶ https://t.co/hlQgu63VK0
*  I find when I get beyond 3 levels of join, it's less confusing  to start encapsulating some of the joins into temporary views.
*  A combination of nested queries, aggregations, and joins - sometimes straightforward to write in raw SQL, but‚Ä¶ https://t.co/kId4Uuobwk
*  Oh I need to be in office to find it, but it was left join of left join of left join of 4 tables and has cut‚Ä¶ https://t.co/67SWPLsUgw
*  I always feel good going through a many-to-many join table.
*  Group wise maximum of a column - https://t.co/YIy03LTgZG, the LEFT JOIN variant.
*  The real one sorted on a much more representative value and also had like three layers of nested joins and ha‚Ä¶ https://t.co/yLDklQkH2D
*  Left join your time series on the date's week number and then running a linear model y~c1+s1+c2+s2 will give‚Ä¶ https://t.co/bx0G3nOlbc
*  Lately I've been really into WITH queries, often combined with subqueries and JOINs into horrifying m‚Ä¶ https://t.co/xC9mN2UT5y
*  Years ago, a lovely one for one of the library services that had 16 unions and at least 8 right joins for eac‚Ä¶ https://t.co/cyCknhN6p1
*  I love writing crazy nested subqueries - I often find them more readable than joins, eg select * from users‚Ä¶ https://t.co/PNdhUR2wwx
*  Using PostgreSQL's generate_series() with cross join to create all possible two-letter combinations! It was f‚Ä¶ https://t.co/xI76Jf6udp
*  I once wrote a page with different types of content. The content was stored in different tables for each type of content. There was one main table which stored which content was where and handled the order and also had a field for the content type. Depending on the type it would Join from the corresponding table and possibly even did subqueries. I ended up with one query that would fetch ALL the content just based on one parameter.  ~80 fields from ~20 tables with ~30 join statements
* Hard: return ticket numbers from a ticketing system if the ticket had a public comment from an employee on it and the ticket was closed. This doesn't sound that hard, but with the way they normalized the data, it was six inner joins!
* Another job, I wrote a library to specify desired columns from multiple CSVs which would form tables to be joined along a specified natural key to form a single final table. The SQL *generation* for loading, keying, joining, and rekeying was... Complicated.
* I did some custom querying inside Drupal once to compare some values across a couple tables. Selecting things from an inner join of some nested custom selections, it looks like? Looking at this now I'm amazed I ever understood it, let alone did it...


cross tab
=========

* Ooh it's been a while but I used to LOVE writing cross tab queries!!! Maybe I'll see if I can hack one together later today after the dust settles in the office.
* This adventure into postgres' crosstab function is something I don't think I could replicate from scratch again: https://t.co/vUPfwktWtg

pivot
=====

* Transposing / pivoting data - so rows become columns and columns become rows - is a bit of of mind bender but can be really useful in some scenarios.
* A fun one is pivotting a key-value table into rows with individual columns for specific keys
* T-SQL lets you have dynamic output schemas, so you can do crazy pivot table queries (like a column per entity in another table's column, sort _columns_ in the result set based on the data, etc).

distances
=========
* I used the Pythagorean theorem to find distances between points listed with latitude and longitude. It‚Äôs not perfect but I was hyped to do it
* Using the Vicenty formula to take lat/long of locations and find the distance between them!
* Another fun one: color search. Find records with rgb values within some distance of a search color, then order by hue so the results look pretty.
* Ordering by distance from a coordinate with latitude and longitude columns in the table.
* Way back when (before GIS support was common in dbs), I did a bounding box on lat/lngs to find homes close to a center point: https://t.co/tpbcbJszq0 It was fun to build, and then I had fun hunting down the performance issues.


business reports
================

*  There was data of 100000+ application in few excel files, i had to import all in standard sql database format with lot of conditions. I write sql queries with 400+ lines to do the task in full dynamic way.... One click and all data imported in related tables  ...  üòé üòé ... Have written some brilliant sql queries for reporting from heavy database. Sometimes proud of that, sometimes looks like i haven't done so much in database...
*  Writing an "aged accounts receivable" report using analytic SQL (model clause, windowing functions, partition‚Ä¶ https://t.co/IpZtx2I36t
*  For an energy company-a report, one line per customer, with all their meter details included, irrespective of the number of meters. So not rectangular. Best bit was listening to the Or*cl* team explain why it wasn‚Äôt possible just before I handed over the T-SQL version.
*  My favorite was one a payroll function that split each employees‚Äôs 401k matching contributions between the pr‚Ä¶ https://t.co/6wdOBkEquN
*  Query that rolled up kpi scores per day into a weekly score and then rolled those up into a 6-weekly score. E‚Ä¶ https://t.co/YgplC8WCQ0

parsing
======
*  I once had to use SUBSTR() and regex to parse out information from HTML stored in a database column.
*  Regex in XML (or DOM Nodes) will always fail. Not as hard as trying to verify RFC compliance for e‚Ä¶ https://t.co/TDlYsN0xAc
*  This one was fun. I needed to parse an XML document and recreate it as a much simpler (and flatter) text file‚Ä¶ https://t.co/PfdfiSa46y

business logic in sql
=====================
*  I wrote a whole blogpost about a SQL query to generate the ladder for a debate competition, which was dramatically faster than doing the same thing in code https://mjec.blog/2016/06/26/your-rdbms-has-cool-features/
* Recurring billing database - I wasn't allowed to use anything but SQL so I wrote a several-thousand character query with dozens of sub-queries with time math and currency conversions

incomprensible
==============

*  Worked for a service provider in the financial space.  We had numerous characteristics that could be assigned groups and in those groups each rep could be assigned to support certain attribures.  The admin console was a pain in the butt to get those queries.






* My favorite was one a payroll function that split each employees‚Äôs 401k matching contributions between the properties the person‚Äôs time was billed to during that pay period, at the same ratio as their billed time instead of 100% from the parent company.
* Also good examples in the {modeldb} R package, with good motivation for having a SQL implementation
* @j_chapper sadly, yes. serialized php is quite difficult to parse using SQL.
* Wrote a search engine UDF from scratch in PL/pgSQL. It implemented the TF-IDF and Cosine Similarity formulas üòÅüëç

Super fun and fairly challenging. https://t.co/7ciQYwIHGm
* given a fixed-length array, transform it into k/v pairs(odd/even) indexes, emitting a row for each pair. We do this quite often when we parse serialized php objects.

Also, please never php-serialize stuff to your data warehouse :D
* @preaction Regex in XML (or DOM Nodes) will always fail. Not as hard as trying to verify RFC compliance for email addresses, but nonetheless: it will fail hard.
* While everybody here is talkong about huge programs and complec stuff that i am yet to achieve, as a beginner, i struggled with GROUP BY because it took me a week to realise we have to use aggregate functions along with it. I hope one day i can do amazing SQL work like y'all. ‚ù§
* @vpicavet I'll like to see this query!
* Beginners seem to have a lot of trouble understanding group by and left join, and their combination. Once you have that, I think the rest is a smop
* We had list of entries that was patient‚Äôs records (immunisations, prescriptions, etc.) that were ordered by time. What we needed to do is to group them into sets that:
- starts with the first entry the day
- ends with the end of the day
Simple, but with a catch.
* My most fun was to visualise processing of multiple tasks at once from their start and end dates. I‚Äôve used it more recently to demonstrate queuing of calls in a server when load reaches a certain level https://t.co/AosCjssOVh of course there are even nicer ways of doing it
* Fist time I built complex json objets from normalized data. Really satisfying
* I once wrote a page with different types of content. The content was stored in different tables for each type of content. There was one main table which stored which content was where and handled the order and also had a field for the content type. Depending on the type it would
* loong ago, I wrote a function to generate CSV from rows in a table, for MSSQL. Useless but fun... ahh to be young :) https://t.co/7eyWg5J6ix
* This is a great SQL problem. It seems simple, but is in fact very complicated and deep.

https://t.co/gohqwTr1k3
* Both hardest and most fun was SQL Pie Chart: https://t.co/7V519jLZ0S

Took away a couple nights sleep :)
* Query that rolled up kpi scores per day into a weekly score and then rolled those up into a 6-weekly score. Especially challenging because I had to understand data model written by another dev that was very confusing to me.

* Also I recently ran an insert query that added ONE BILLION records to a table. I was expecting it to take ages but redshift didn‚Äôt even break a sweat, it was done in 13 minutes. https://t.co/EEW4x4L3J0
* I once implemented a Turing machine in Oracle SQL https://t.co/1Jgzv4QUrc
* This is really basic but whenever I use SELECT... EXCEPT I always feel a fresh wave of amazement at how great it is that I don‚Äôt need to analyse the results myself. Especially on redshift, where it‚Äôs blisteringly fast.
* @tapoueh Hmm sounds like gaps and islands, which could be done with a single pass using row_number instead of the self join or recursion...

Of course, if we had match_recognize in PostgreSQL... :)
* @TanelPoder Hah üòÅ
* my first part time job was about entering lot of data. And I has SQL access to them.

About 1KB query over 7 tables where some of them were used 3 times due to database structure and my low SQL skills.
* Here I am wondering if it really has to be SQL or if re-implementing InnoDB data structures in Ruby counts

https://t.co/Xb7SWs8kbC
* query for a grid: involves objects, dependent objects that might exist, and the most recent detail of the set of dependents. Joins and subqueries, sql case statements, and functions.
* https://t.co/hsxgaVMWxH this one was pretty fun - expanding a list of repeated transaction schedules into a list of real transactions
* @chriseng What the heck is this? :) https://t.co/beZJGTWxGw
* @majelbstoat I didn't write it, but I have one in a project that does cartesian to polar coordinate transformations to give you a bearing and distance
* A triangular heatmap, using a synthesized column for a counter, all in one query. SQL is magic.
* Big data multicursor calculation stored procedures.
* Extracting the list of intern applicants who had completed our take-home assignment. @transposit SQL for APIs FTW https://t.co/M1r8MAgLYE
* The most important query I‚Äôve ever written produces a fish. Equally importantly, the query itself looks like a fish too. Runs on Oracle.

https://t.co/HlWPeBlxDH https://t.co/NQdoqPKaRO
* This one was pretty hard; trying to identify and reconcile differences between two databases where we only had direct access to one.
https://t.co/V72rOg6UEV

The original check ran once per day and took ~27 hours (doing a line by line comparison)
The replacement took an hour
* @webKnjaZ tsql OVER(...) is a trip -- I remember some really fun queries with that https://t.co/93jEDfZ7nC
* I learned to use ‚ÄúWITH RECURSIVE‚Äù to speed up daily aggregation over a largeish data set when the query planner wanted to do full table scans and sorts when just using group by and order by
* I had one the other day! Select the rows that are duplicates but keep the oldest record.

The query was:

Select https://t.co/cMd5gIPcTr from ( select row_number() over (partition by user_id order by created_at) as rid, id from table ) x where x &lt;&gt; 1;
* @mojodna And then there's just art to be made with SQL: https://t.co/U38JL4u3Wh
* @mojodna @USGSNHD This is larger and more complicated than mine, but somewhere I have some hydro tracing using recursive CTEs (no nesting though). I love this.
* Recursive K means clustering... it broke production (query on dedicated analytics server blocked writes from replication, which caused the prod db to cache and run out of memory)
* @smathermather This one navigates down rivers (NHDPlus from @USGSNHD) and constructs lines between snapped points. Spatial joins and nested recursive CTEs, woohoo!

https://t.co/CiM1W6D5wg
* @mojodna And this one was simply abuse of a database, but a fun thought exercise: https://t.co/qwLqcNYW5E
* This was significantly wackier and inspired by something @mojodna was playing with at the time: https://t.co/RuSv1oRzNN
* This one was also entertaining (and valuable): https://t.co/1ZZNs2aWoo
* This one was fun. I needed to parse an XML document and recreate it as a much simpler (and flatter) text file as input for a process, so a little xpath, a little SQL: https://t.co/gDAiScPX7J
* @databasescaling Rank over partition by FTW!
* generating positions of bids, if there is a tie at the start the position starts at 2 https://t.co/2v1DL7192V
* Recursion select in postgres to build a tree of category levels from last child up to the root.

Did it once. Don‚Äôt remember how but it was one query with acceptable performance.
* The most fun SQL query I ever wrote was the first one. It wasn‚Äôt fancy, but it felt like I was pulling back the curtains to find how things really worked. It‚Äôs up there with the first time I used a Telnet client to send an HTTP request.
* I had to write some interesting queries on Google BigQuery when I was at Yelp to pull analytics data for experiment analysis because the tean didnt have mature tools for that.  Gets tricky to query analytics events, aggregate them, etc
* A convenient pattern that applies to any gnarly query is to then do CREATE VIEW report_gnarly_query AS &lt;text of gnarly query, including any ORDER BY, LIMIT, and column renaming&gt;. Then all day-to-day queries turn into SELECT * FROM &lt;view_name&gt;.
* A huge CASE with a ton of regexps in the conditions. To aggregate web stats for a complex directory tree into a reasonable number of buckets (installation, support, product X, Y, or Z, etc.) based on combination of path, directory name, filename.
* @pdglenn I remember this!
* Lately, I had to format existing data in a DB, my first idea was to read, store in a temporary variable then update previous data. But a query which implemented find and replace worked like magic
* Looking at sequences of data points to filter spikes and other anomalies.

Loaded prior and next 3 data points, used a massive list of "patterns" (generated from another complex query using sample data), and operated as a set operation on thousands of rows every few seconds.
* An Union select  (SQLi) on a project form, which led to a complete db dump of the website
* I wrote SQL to find if string exists anywhere in database with ~1000 tables and replace it in-place with anonymized, equal length/structure string (phone numbers, ssn:s etc).
I got familiar with metamodel queries and dynamic sql, and had queries with horrible execution times.
* @l_avrot @DataRenee that's super cute!
* In Odoo v8 reporting pages had to be written directly as SQL views, all an overly complex query. I left 2 comments: "abandon all hope, ye who enter here" at the beginning and "they're not paying you enough" at the end.
* Making a query that takes 30 seconds take 0.05 seconds, and use 1/500,000th the resources.
* @DataRenee The more fun was my first one with my daughter (8 y-o at the time), when she didn't wanted to go bed and used SQL learning as an excuse:

Values('Bonne nuit Roxane);
* note, this used to be "give me all the ones that run at this hour", but that didn't work out when we skipped an hour, so we changed it to be "how overdue is each one"

that was the tricky part
* scheduler:

given a table of backup, time of day, last_backup

return an ordered list of all of them by how overdue they are
* Not one, but a set of 200 queries we wrote for a research project https://t.co/mpUR2mU0JQ (long time reader, first time reply-er, thanks for the great work you do + share!)
* @BarryV The struggle (TM)
* @stuwest I thinkk I am looking for this one as we speak! I gotta try to adapt it to my situation, but I like it already. Thanks @stuwest for sharing and for asking!
* @sharlagelfand we should swap code! I started doing it on BigQuery and it's pretty lovely, but I always do the last step in Excel!
* I had a propriatary API, you give it a tag and timestamp, it gives you value at that time. I needed a JSON API. So, I wrote a query to transform the table into JSON object string and return that as the value. It was dope.
* @thebigjc woof YES i have written some 400+ line queries for retention
* Definitely a recursive common table expression. Essentially just cribbed it from the SQLite docs and tweaked for my case. Used it to build a tree structure from rows with (id, parent_id, ...).
* Some fun ones were a recursive query to find digits of pi,  a 501 darts ‚Äúout‚Äù calculator, and one I submitted to an obfuscated code contest.
* A simple but handy bit is to turn numbers into histograms by running them through eg. mysql's repeat().
Something like:
select date, repeat('*', count(*)/10) from transactions group by date
This crappy little thing helped me spot anomalies way more often than i'd like to admit.
* Calculating churn and retention. It's all set math and works well on SQL but is just hard enough that I have to relearn things when I do it.
* @pr0zac @simonw Did you find it less readable? What did you replace it with? One thing I wanted with SQL was something more composable--and it seems like this is one way to do it.
* Long ago but I had to write a query to display a calendar of the current month
* I had an assignment in school to calculate PageRank on Wikipedia pages using SQL that was pretty wild
* 1. building a histogram 2. diffing a table against a backup of itself to see what values changed
* @rustamrocks 2 real 4 me
* This is traffic and congestion simulation, again entirely implemented using a recursive SQL query.

(Awesome web-based animation by @Doersino.)

https://t.co/orUt428YMd
* https://t.co/03XLTPOwZ3
* @eeJonathan If that's enough to get me into the Salty Spittoon, getting me drunk and asking me about that company is enough to hand me the deed.
* ...and bit by bit I added to my query until I finally had everything I wanted. I was particularly pleased with setting string values for the options from nullable booleans and three outer joins (having mostly used inner joins in the past). Probably super simple, but I was pleased
* I had a really badly performing application (using linq to access sql data and joining lots of datasets together in code). I thought I might be able to build a single SQL query to provide me all the data in exactly the right format 1/
* The ones where you have a compound key, the 2nd part being a version number and you need to work out the best way to get only the latest (or a specific, or a constrained) version of all the other columns in the same table.
* I once had to write some meta-sql to profile tables in the database, to get the names and types of columns, primary and foreign keys etc. Really interesting
* Nobody mentioning parsing json objects to move from an unstructured format to a real db format after poc is a crime üòÄ
* I wrote a book recommendation algorithm primarily in SQL: Lots of joining to find "similar" users (people who rated your favorite books highly) and take their favorites you hadn't read. :)
* @sunahsuh WWWWWWWWOW!
* I cannot now recall what interesting I've written when working in a company that had WHOLE business logic in the db, but these were definitely more hard than fun, and mostly ugly and a terrible idea tbh
* Accidentally combined many complex views to implement multi-feature linear regression to predict optimal orders of rental uniform inventory

Of course, I didn't know what any of that meant at the time, nor that SQL was totally the wrong tool for the job

Worked tho!
* Building the entire schedule for @phpconferencebr in a single query: dates, slots, rooms, speakers, presentation titles, presentation categories, the whole lot.

I'll probably be able to find it somewhere in my HD (of course I've saved it =) )
* This one was particularly entertaining and also ended up being a good excuse to practice some Oracle features that have always annoyed me https://t.co/MVlq2x0L5q
* Writing an "aged accounts receivable" report using analytic SQL (model clause, windowing functions, partitioning, etc.). The initial query took a day or two, but it was buggy. Getting the bugs out took another week.
* Anytime I use Common Table Expressions I feel like a magical wizard üßô‚Äç‚ôÇÔ∏è ‚ö°Ô∏è
* @eatheringtonp I feel this.
* Example from today: select payload-&gt;&gt;'x' as x, array_agg(b-&gt;&gt;'c') as b from some_table, jsonb_array_elements(payload-&gt;'a'-&gt;0-&gt;'b') as b group by x;
* Weekly scores for a national pub trivia competition. Full list of team name + date + region + event rank -&gt; team id pulled from name (multiple variations), aggregate score for the week, ranking by region.
* Plus all these funny operators like -&gt;&gt;, @&gt;, #&gt; are confusing at the beginning but so useful when regularly using them!
* @simonw I really like this one, used to audit a dataset. It uses CTEs to run various test queries and hstore to pivot them to a nice tabular report format: https://t.co/DsGRuvU3n1
* It would be something querying JSON(B) columns in PostgreSQL. Hard/fun (I cannot decide) bit there is that I have json structure on top of db structure and queries need to dig into both.
* PARTITION BY...OVER in SQL Server to get the top item from many subsets
* This one made me cackle a lot
https://t.co/eDaMg208Bz
* For this year's @PostgresVision conference, I presented a query for finding how much more generous at tipping are customers being picked up at @DeadRabbitNYC vs those being dropped off there, using the NYC public taxi data archive. üöï
* @TheCravenOne This is like something you would say to get in to the Salty Spitoon.
* https://t.co/dQkZGGeeEU
* A dynamic database, had only two tables and created new ones with the rows the user wanted.
Used this for a mobile project in college like 6 years ago.

https://t.co/Ui2cNiRcdh
* @simonw Same for WITH expressions. Sometimes has a performance hit wrt query optimisation, but infinitely more readable and less repetitive
* @pr0zac I love those! So much fun building up a bunch of useful derived tables and then joining them all together (or using them for big IN subselects)
* Identifying non-disjoint causal factors in failure over multiple task execution attempts for the same entity, but separating by whether the entity ever had a successful execution attempt, and the table is over the attempts with the entity as a combination of attributes
* My first query in postgres after nearly 20 years of using MySQL.
* Resently revisited some old code of mine that I then rewrote from running 126 calls to the db, each doing a count of the result of a where. Rewritten to a single dB call having 126 cols each running a sum + case then 1 else 0
* A function that returns a row set like this:

select * from numbers(5):

   N
====
    1
    2
    3
    4
    5

Useful for joining on, like:

select * from line_items cross apply numbers(line_items.qty)

It turns a table of rows with a quantity into a table with 1 row for each
* Left join your time series on the date's week number and then running a linear model y~c1+s1+c2+s2 will give a two period seasonal trend prediction. Expand periods (and/or date range, say month or day level) as needed, each period is an additional "hump" in your timeseries.
* It was the first time that I used the Geography data type in SQL server 2008 R2. I was given the co-ordinate data of England &amp; Scotland on one scale and Northern Ireland and Wales on a 50x smaller scale. 2 weeks of effort from my side. My query finally United the Kingdom!
* I tapped into the events and create files if drop create alter occur. That was nice. Sort of versioning control
* I just put a while loop in a stored procedure a few minutes ago and I'm probably going to hell now
* @mwotton Costed two weeks of my life (lots of research and experiments). The award was amazing - finally felt I'm not a junior developer anymore üî•
* @mwotton You reminded me of a stored procedure I had to write a looong time ago. It involved combination of results from full text search + filesystem index search (searching through attachments stored in directories).
* @simonw Lately I've been really into WITH queries, often combined with subqueries and JOINs into horrifying monstrosities. Last night at like 11:30pm I put together something similar to this. Decided this morning I should prob work out a saner way to accomplish the same thing. https://t.co/xILGZRomuz
* Week timeseries 2 period Fourier in standard SQL:
with
y_in_wk as (SELECT y FROM UNNEST(GENERATE_ARRAY(1, 53)) AS y)
select y as wk_of_yr,
sin(2*ACOS(-1)*1*y/53) as S1,
cos(2*ACOS(-1)*1*y/53) as C1,
sin(2*ACOS(-1)*2*y/53) as S2,
cos(2*ACOS(-1)*2*y/53) as C2
from y_in_wk
* Calculating a ‚ÄúTaboo‚Äù score vs friends‚Äô after several rounds of the player choosing from one of two cards depicting taboo human behaviors from different cultures around the world. There were less than 100 cards total so one table in the DB kept stats on each combination.
* An evil thing using lots of date_add and date_format to get the days/dates for a week on a timesheeting system. Ugly as chuff, but worked fine across every possible date-altering eventuality, and for doing a week from any day, rather than fixed Mon-Sun (or Sun-Sat) weeks
* Using PostgreSQL's generate_series() with cross join to create all possible two-letter combinations! It was for a specific business report that used these code to identify customers. https://t.co/9GVABmBhNh
* INSERT ON DUPLICATE KEY UPDATE (with if elses for the update values)
* Migrating a PK from VARCHAR(255) to BINARY(16), since in practice it was only ever a 128bit UUID, was satisfying. Mostly fun because our vendor version didn‚Äôt provide a built in conversion function.
* whoops cases are hard! should be "if the prev set and next set of events on this cookie have the same user id but this set doesn't, combine into one session"
* Anything with nested subqueries as temporary tables. It's a "look what I can make it dooooo" thrill
* @librarieshacked SPARQL more satisfying to write than SQL - I really love conquering a complex query in SPARQL but with SQL it always just feels like a chore (not that I write much of either tbh)
* Definitely an auto-validation tool for some self-service reporting products my company makes. Trying to decide when a tool is "set up" correctly based on the tables they have in system is tough.
* Adding session ids to events! It required some window functions and cases for "if the previous set of events didn't have a user id but this set does and the next set has the same one, then combine into one session" etc.
* Once I wrote an SQL to update every order line number to start with 9000 and increment by one except the first order line no.. i had to use the same table to update.. used merge statement with ranks, partition by, etc.. etc.., was very happy when it worked like a charm.. :)
* been writing some horrible but enjoyable spatial SQL queries for calculating the location on a road of a mobile library based upon its timetable and associated route lines between stops.
* Mine was... Just a simple case if else then query that got the exact result that my teammate needed...
* Years ago, a lovely one for one of the library services that had 16 unions and at least 8 right joins for each one, to get out all the info for one report. Used to love doing string slicing in the SQL as well.
* Does a program converting a certain type of stuctured input into an sql query for system monitoring count?
* I turned an itemization table into about 50 business metrics. It was a ‚Äúboring‚Äù project, but it taught me so much about what analysts care about, how to make queries performing and efficient, and generally what drives business decisions in terms of fundamental data.
* I'm loading lots of small (tens of thousands of rows) datasets into SQLite these days for personal analytics experiments and this kind of query works great for that too - https://t.co/De1Loc05WM
* I used to avoid that kind of thing like the plague for performance reasons - the I realized that it's completely OK for:

* data warehouse queries that are compiled to map/reduce and can take 30s just fine
* analytics queries against tables with only a few hundred thousand rows
* I love writing crazy nested subqueries - I often find them more readable than joins, eg

select * from users where id in (
   select uid from facebook_accounts
 ) and id in (
   select uid from twitter_accounts
)

For users who have connected their account to both FB and Twitter
* I wrote a massive query that fed a solr search index. Maintaining it was difficult, but the configuration on the solr side was easier.
* @derekprior I miss you talking about that stuff on @_bikeshed, (@christoomey has been doing an amazing job over there though)
* The real one sorted on a much more representative value and also had like three layers of nested joins and handled linkage from top level records to a different record type, honestly it's probably just as well I couldn't find it.
* For a recent example I would use https://t.co/Yu4Eg5we4B I think...
* (and then if you want to make sure there are no date gaps you have to generate a table with all dates and join to that,,, and then six hours later you‚Äôre still at your desk covered in Cheetos dust and nursing a Diet Coke hangover)
* There was data of 100000+ application in few excel files, i had to import all in standard sql database format with lot of conditions. I write sql queries with 400+ lines to do the task in full dynamic way.... One click and all data imported in related tables  ...  üòéüòé
* Aggregating across multiple ways of grouping a dataset using cube(), it replicated what we'd been doing in a java program in a few minutes to a couple of seconds.
* @Doersino I'm confused, terrified, and interested
* This runs and is as far as I can tell correct!

https://t.co/ntKOGuMe1l
* I learned a lot trying to write this. I'd still like to clean it up one day, but I'm trying to avoid self-nerd-sniping on this side project

https://t.co/3jYkoOmOui
* Distributed Transaction across heterogeneous XA Compliant datasources ... quite challenging !!!
* Home bake version of the queries that underpin SMO before I knew what SMO was. Used them to build a query that spat out a prettified ‚ÄúCREATE TABLE‚Äù script. Ran a whole reactive version control module with SQL agent and xp_cmdshell üôÉ
* You have the best attitude b0rk. I appreciate you.
   üòé
üëâüëâ
* I don't have a copy but it was a recursive common table expression that built an flattened lookup for all levels of a deeply nested tree (get arbitrary-level parent or children of ID with type X). Needed it to integrate complex reporting into an inflexible reporting system.
* And the only reason I got to writing it in SQL was RoR's inability yo properly construct these third-order associations.
* One more and I'll hush: learning about oracle meta-tables and views was super useful for monitoring things like lock contention, and a good party trick when on call issues came up and I knew how to tell what application components were doing what to the database at a given moment
* Update customer set balance = 0

/** There. Just like that. In production. WITH NO WHERE clause. It was undeniable fun
U_U **/
* Cumulative totals was a bit of a lightbulb moment (the rough form is SELECT COUNT(*), https://t.co/RQo1QCuPPY FROM t1 INNER JOIN t2 ON https://t.co/RQo1QCuPPY &gt;= https://t.co/QGPSKdPSFS GROUP BY https://t.co/RQo1QCuPPY)
* Hardset - set based insert on multiple tables (if I remember correctly - four)  involving triggers, data flow and correct referential integrity between them :) Writing asynchronous logic (with queues) is fun as well:) But I love the most writing simplified SQL with CTEs.
* The hard ones involved reconstructing when commits from multiple sources were deployed to each stage of a release process given the history of all commits, the history of all deployments, and the tip commits in each deployment
* Identify all rows associated with a single session, get starting and ending times, use those to establish max concurrency of sessions in a given variable date/time range
* For a while I was writing analytic models using Hive only . Which is an SQL dialect for a big data computation Engine.  I have seen some crazy craaaaazy shit. Lots of carving out specific rules for who is included and excluded based on something or other.
* Execution plan made by SQL Server at compile time becomes an issue as the data grows in size. It's called 'Parameter Sniffing'. Just a recompile dropped the execution time from few hours to few seconds.
* Gosh I feel like this question is catnip for me because I secretly love optimising sql queries... But I also think complex queries are inherently awful no good bad code smells
* Back in 2004, I wrote this really long UPDATE statement that used deeply nested IFF()s to combine multiple updates into a single statement. Ended up exceeding either the network buffer size or the MySQL TCP buffer or something like that, but it was faster than a loop.
* Portfolio value over time - computing the sum of shares multiplied by the value of each on each day and making that performant
* The hardest SQL queries I've had to write usually involved scrubbing dirty data that had many nested foreign key relationships.
* I wrote a query that looked for entropy in numeric values in transactions across groups of buyers and sellers (basically looking for: is this a regularly repeated dollar amount per transaction, or is there high variability in dollar per transaction with these buyer/seller combos)
* A query in a redash dashboard that aggregated timeseries data from Spark jobs. One of the columns returned was a valid link to a REST API that rendered an image of the jobs' resource usage over time as little sparklines. Worth it for the pun alone!
* Once I wrote a stored procedure to convert between alphanumeric text and Morse Code. It was to test how far "solve this coding challenge in any language you want" could be stretched.
* I‚Äôve always loved writing statements that insert a record into one table and extract the generated id to insert it into a foreign key‚Äôd table. Always a confusing adventure
* elasticsearch-alike that converts an ADT into a postgresql full-text-search and faceted search via jsonb operators.
* select [select],*,star as [as] from [from],[order] order by [by]
